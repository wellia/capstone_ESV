{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "config = {\n",
    "    'FILE_PATH': 'cleaned_incidents1.csv'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import string\n",
    "    import re\n",
    "    from time import time\n",
    "    import texthero as hero\n",
    "    from texthero import preprocessing\n",
    "    from gensim.models import Word2Vec\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.tree import DecisionTreeClassifier \n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    from sklearn.feature_selection import chi2, SelectKBest\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    import warnings\n",
    "except(ImportError):\n",
    "    print(f'Import Error: {ImportError}')\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# set seeds for reproducability\n",
    "from numpy.random import seed\n",
    "seed(500)\n",
    "\n",
    "# global configurations\n",
    "pd.set_option(\"display.max_colwidth\", -1) # show larger text in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df = pd.read_csv(config['FILE_PATH'])\n",
    "\n",
    "new_df = df[['EventDescription', 'FailedAssets', 'IncidentCause', 'IncidentConsequence', \\\n",
    "             'IncidentType', 'Status', 'WeatherStation', 'CauseEnvironment', 'CauseTechnical', \\\n",
    "             'CauseCommunity', 'ActionTaken', 'Category', 'IncidentLocationType', 'NetworkType', \\\n",
    "             'WeatherStation', 'Locality']]\n",
    "\n",
    "new_df = new_df.dropna(axis=0, subset=['Category'])\n",
    "new_df = new_df.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ActionTaken', 'Address', 'AssetLabel', 'CauseCommunity',\n",
       "       'CauseEnvironment', 'CausePre', 'CauseTechnical', 'CauseWorkP',\n",
       "       'ContactType', 'CorrectProtection', 'EventDescription', 'FailedAssets',\n",
       "       'FailedExplosion', 'FailedOilFilled', 'FailedOtherAssets',\n",
       "       'FailedOtherAssetsOther', 'FeederNumber', 'IncidentCause',\n",
       "       'IncidentConsequence', 'IncidentDatetime',\n",
       "       'IncidentFireFFactorReportable', 'IncidentFireSeverity', 'IncidentID',\n",
       "       'IncidentLocationType', 'IncidentLocationTypeOther', 'IncidentNumber',\n",
       "       'IncidentType', 'Lat', 'Long', 'MadeSafe', 'NetworkType', 'Status',\n",
       "       'SubmissionID', 'SubmittedDateTimeString', 'Voltage', 'WeatherStation',\n",
       "       'Postcode', 'Locality', 'Category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Text Cleaning and Pre-processing\n",
    "def preprocess_text():\n",
    "    # cleaning steps\n",
    "    cleaning_pipeline = [\n",
    "        preprocessing.fillna,\n",
    "        preprocessing.lowercase,\n",
    "        preprocessing.remove_whitespace,\n",
    "        preprocessing.remove_punctuation,\n",
    "        preprocessing.remove_urls,\n",
    "        preprocessing.remove_brackets,\n",
    "        preprocessing.remove_stopwords,\n",
    "        preprocessing.remove_digits,\n",
    "        preprocessing.remove_angle_brackets,\n",
    "        preprocessing.remove_curly_brackets,\n",
    "        preprocessing.stem\n",
    "    ]\n",
    "\n",
    "    # apply pipeline to text\n",
    "    clean_text = features.pipe(hero.clean, cleaning_pipeline)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def get_lematizer(sentence):\n",
    "    clean_text =  (\" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence) if w not in string.punctuation]))\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Tfidf: (6489, 12364)\n",
      "finish feature selection:  (6489, 5000)\n",
      "DT: 0.7350585335797906\n"
     ]
    }
   ],
   "source": [
    "# EDA Experiments\n",
    "\n",
    "# Get features\n",
    "features1 = new_df['CauseCommunity'] + ' ' + new_df['CauseEnvironment'] + ' ' + new_df['CauseTechnical'] + \\\n",
    "    ' ' + new_df['EventDescription'] + ' ' + new_df['FailedAssets'] + ' ' + new_df['IncidentCause'] + \\\n",
    "    ' ' + new_df['IncidentConsequence'] + ' ' + new_df['IncidentType'] + ' ' + new_df['ActionTaken']\n",
    "        \n",
    "features2 = new_df['EventDescription'] + ' ' + new_df['FailedAssets'] + ' ' + new_df['IncidentCause'] \n",
    "\n",
    "features3 = new_df['CauseCommunity'] + ' ' + new_df['CauseEnvironment'] + ' ' + new_df['CauseTechnical'] + \\\n",
    "    ' ' + new_df['EventDescription'] + ' ' + new_df['FailedAssets'] + ' ' + new_df['IncidentCause'] + \\\n",
    "    ' ' + new_df['IncidentConsequence'] + ' ' + new_df['IncidentType'] + ' ' + new_df['ActionTaken'] + \\\n",
    "    ' ' + new_df['IncidentLocationType'] + ' ' + new_df['NetworkType'] + ' ' + new_df['Locality']\n",
    "\n",
    "# change here to experiment with different features\n",
    "features = features1\n",
    "\n",
    "target = new_df['Category']\n",
    "    \n",
    "# clean the data\n",
    "clean_text = preprocess_text() # feature extraction/vectorize\n",
    "\n",
    "# lemmatization - the process will take a while and do not increase the accuracy\n",
    "# uncomment it if you want to experiment with this\n",
    "#clean_text = clean_text.apply(lambda sentence: get_lematizer(sentence))\n",
    "\n",
    "# preparation for feature selection\n",
    "new_df['category_id'] = new_df['Category'].factorize()[0]\n",
    "category_id_df = new_df[['Category', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "#print(category_to_id)\n",
    "\n",
    "# vectorize\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2))\n",
    "\n",
    "tfidf_features = tfidf.fit_transform(clean_text).toarray()\n",
    "labels = new_df.category_id\n",
    "print('After Tfidf:', tfidf_features.shape)\n",
    "clean_text = tfidf_features\n",
    "\n",
    "# feature selection - set num_of_selection to > 0  if you want to do feature selection \n",
    "# based on to find the terms that are the most correlated with each of Category\n",
    "\n",
    "num_of_selection = 5000 \n",
    "if num_of_selection > 0:\n",
    "    ch2 = SelectKBest(chi2, k=num_of_selection)\n",
    "    clean_text = ch2.fit_transform(tfidf_features, new_df['category_id'])\n",
    "    print('finish feature selection: ', clean_text.shape)\n",
    "\n",
    "# # split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(clean_text, target, random_state=0, test_size=0.25, shuffle=True)\n",
    "\n",
    "# classification - DT\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(x_train ,y_train)\n",
    "y_pred = dt.predict(x_test)\n",
    "print('DT:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# classification - SVM\n",
    "svc = SVC(C=1.0, kernel = 'linear')\n",
    "svc.fit(x_train, y_train)\n",
    "y_pred = svc.predict(x_test)\n",
    "print('SVM:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "CauseCommunity, CauseEnvironment, CauseTechnical, EventDescription, FailedAssets, IncidentCause, IncidentConsequence\n",
    "IncidentType, ActionTaken. Use all features. SVM: 84.47%, DT: 73.75%\n",
    "\n",
    "Initial features after vectorisation: 12364\n",
    "\n",
    "As above, but with feature selections 1000. SVM: 83.05%, DT: 73.14%, the SVM classification is much faster\n",
    "\n",
    "As above, but with feature selections 3000. SVM: 83.06%, DT: 73.57%\n",
    "\n",
    "As above, but with feature selections 5000. SVM: 84.47%, DT: 73.50%\n",
    "\n",
    "EventDescription, FailedAssets, IncidentCause. No feature selections. SVM: 83.73%, DT: 71%\n",
    "\n",
    "CauseCommunity, CauseEnvironment, CauseTechnical, EventDescription, FailedAssets, IncidentCause, IncidentConsequence\n",
    "IncidentType, ActionTaken, IncidentLocationType, NetworkType, Locality. DT: 74%. SVM: %84.17\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'AF Other':\n",
      "  . Most correlated unigrams:\n",
      "       . compon\n",
      "       . capacitor\n",
      "       . hps\n",
      "       . light\n",
      "       . lantern\n",
      "  . Most correlated bigrams:\n",
      "       . report street\n",
      "       . light lantern\n",
      "       . public light\n",
      "       . light fire\n",
      "       . street light\n",
      "# 'Animal':\n",
      "  . Most correlated unigrams:\n",
      "       . magpi\n",
      "       . nest\n",
      "       . flashov\n",
      "       . possum\n",
      "       . bird\n",
      "  . Most correlated bigrams:\n",
      "       . caus flashov\n",
      "       . found bird\n",
      "       . bird contact\n",
      "       . bird unknown\n",
      "       . bird flashov\n",
      "# 'Conductor':\n",
      "  . Most correlated unigrams:\n",
      "       . span\n",
      "       . splice\n",
      "       . rusti\n",
      "       . clash\n",
      "       . spreader\n",
      "  . Most correlated bigrams:\n",
      "       . clash grassfir\n",
      "       . bare conductor\n",
      "       . conductor broken\n",
      "       . wind conductor\n",
      "       . conductor clash\n",
      "# 'Connection':\n",
      "  . Most correlated unigrams:\n",
      "       . shower\n",
      "       . corrod\n",
      "       . box\n",
      "       . overh\n",
      "       . connect\n",
      "  . Most correlated bigrams:\n",
      "       . conductor termin\n",
      "       . servic neutral\n",
      "       . neutral connect\n",
      "       . connect box\n",
      "       . overh connect\n",
      "# 'Crossarm':\n",
      "  . Most correlated unigrams:\n",
      "       . wood\n",
      "       . cross\n",
      "       . arm\n",
      "       . leakag\n",
      "       . crossarm\n",
      "  . Most correlated bigrams:\n",
      "       . leakag current\n",
      "       . damag crossarm\n",
      "       . crossarm fire\n",
      "       . cross arm\n",
      "       . crossarm wood\n",
      "# 'Dug up':\n",
      "  . Most correlated unigrams:\n",
      "       . bore\n",
      "       . excav\n",
      "       . dug\n",
      "       . contractor\n",
      "       . underground\n",
      "  . Most correlated bigrams:\n",
      "       . work close\n",
      "       . cabl excav\n",
      "       . cabl contractor\n",
      "       . underground cabl\n",
      "       . close underground\n",
      "# 'Fuse':\n",
      "  . Most correlated unigrams:\n",
      "       . acid\n",
      "       . boric\n",
      "       . fuse\n",
      "       . edo\n",
      "       . candl\n",
      "  . Most correlated bigrams:\n",
      "       . fuse tube\n",
      "       . boric acid\n",
      "       . fuse candl\n",
      "       . candl fuse\n",
      "       . edo fuse\n",
      "# 'Installation':\n",
      "  . Most correlated unigrams:\n",
      "       . hopper\n",
      "       . poa\n",
      "       . condition\n",
      "       . board\n",
      "       . unidentifi\n",
      "  . Most correlated bigrams:\n",
      "       . attend suppli\n",
      "       . poa connect\n",
      "       . custom side\n",
      "       . custom fault\n",
      "       . unidentifi custom\n",
      "# 'Lightning':\n",
      "  . Most correlated unigrams:\n",
      "       . struck\n",
      "       . 15pm\n",
      "       . applianc\n",
      "       . strike\n",
      "       . lightn\n",
      "  . Most correlated bigrams:\n",
      "       . lightn custom\n",
      "       . strike pole\n",
      "       . wood lightn\n",
      "       . lightn struck\n",
      "       . lightn strike\n",
      "# 'OH Cable':\n",
      "  . Most correlated unigrams:\n",
      "       . tap\n",
      "       . perfor\n",
      "       . hvabc\n",
      "       . neutral\n",
      "       . screen\n",
      "  . Most correlated bigrams:\n",
      "       . servic minor\n",
      "       . aluminium neutral\n",
      "       . screen conductor\n",
      "       . screen servic\n",
      "       . neutral screen\n",
      "# 'Other':\n",
      "  . Most correlated unigrams:\n",
      "       . pit\n",
      "       . vandal\n",
      "       . stolen\n",
      "       . copper\n",
      "       . theft\n",
      "  . Most correlated bigrams:\n",
      "       . earth earth\n",
      "       . vandal copper\n",
      "       . unknown person\n",
      "       . theft damag\n",
      "       . copper theft\n",
      "# 'Pole':\n",
      "  . Most correlated unigrams:\n",
      "       . current\n",
      "       . top\n",
      "       . wood\n",
      "       . excess\n",
      "       . leakag\n",
      "  . Most correlated bigrams:\n",
      "       . excess leakag\n",
      "       . wood excess\n",
      "       . pole wood\n",
      "       . pole fire\n",
      "       . found pole\n",
      "# 'Trees':\n",
      "  . Most correlated unigrams:\n",
      "       . wind\n",
      "       . fallen\n",
      "       . veget\n",
      "       . branch\n",
      "       . tree\n",
      "  . Most correlated bigrams:\n",
      "       . tree fell\n",
      "       . bare tree\n",
      "       . branch veget\n",
      "       . blown branch\n",
      "       . tree branch\n",
      "# 'UG Cable':\n",
      "  . Most correlated unigrams:\n",
      "       . learn\n",
      "       . jackson\n",
      "       . pit\n",
      "       . protrud\n",
      "       . trace\n",
      "  . Most correlated bigrams:\n",
      "       . electr pit\n",
      "       . precipit moistur\n",
      "       . damag pad\n",
      "       . pit underground\n",
      "       . ingress electr\n",
      "# 'Vehicle':\n",
      "  . Most correlated unigrams:\n",
      "       . struck\n",
      "       . crane\n",
      "       . hit\n",
      "       . truck\n",
      "       . vehicl\n",
      "  . Most correlated bigrams:\n",
      "       . close overhead\n",
      "       . vehicl struck\n",
      "       . vehicl custom\n",
      "       . unknown vehicl\n",
      "       . truck contact\n"
     ]
    }
   ],
   "source": [
    "# show which terms related to each category\n",
    "N = 5\n",
    "for Category, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2 = chi2(tfidf_features, labels == category_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "  print(\"# '{}':\".format(Category))\n",
    "  print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:])))\n",
    "  print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Experiements##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
