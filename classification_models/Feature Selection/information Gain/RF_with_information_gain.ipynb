{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'FILE_PATH': '/Users/pradeep/Desktop/ProjectANotebooks/notebooks/processed_dataset.csv'}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configuration\n",
    "config = {\n",
    "    'FILE_PATH': '/Users/pradeep/Desktop/ProjectANotebooks/notebooks/processed_dataset.csv'\n",
    "    }\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# import libraries\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import string\n",
    "    import re\n",
    "    from time import time\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    import texthero as hero\n",
    "    from texthero import preprocessing\n",
    "    from gensim.models import Word2Vec\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import StackingClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    import warnings\n",
    "except(ImportError):\n",
    "    print(f'Import Error: {ImportError}')\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# set seeds for reproducability\n",
    "from numpy.random import seed\n",
    "seed(500)\n",
    "\n",
    "# global configurations\n",
    "pd.set_option(\"display.max_colwidth\", -1) # show larger text in pandas dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "'''\n",
    "- select features\n",
    "- drop missing values\n",
    "- combine selected features into one\n",
    "'''\n",
    "def data(df):\n",
    "    #new_df = df[['EventDescription', 'FailedAssets', 'IncidentCause', 'IncidentConsequence', 'IncidentType', 'Status', 'WeatherStation', 'Category']]\n",
    "    #new_df.dropna(axis=0, inplace=True)\n",
    "    features = df['EventDescription'] +' ' + df['Address'] + ' '+ df['IncidentCause'] +' ' + df['ActionTaken'] +\\\n",
    "    ' '+ df['FailedAssets'] +' ' + df['Locality']  + ' ' + df['IncidentConsequence'] + ' ' + \\\n",
    "    df['CauseTechnical'] + ' ' + df['CauseTechnical']\n",
    "    target = df['Category']\n",
    "    return features, target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# read csv\n",
    "dataset = pd.read_csv(config['FILE_PATH'])\n",
    "\n",
    "# get features and target\n",
    "features, target = data(dataset)\n",
    "\n",
    "# Text Cleaning and Pre-processing\n",
    "def preprocess_text():\n",
    "    # cleaning steps\n",
    "    cleaning_pipeline = [\n",
    "        preprocessing.fillna,\n",
    "        preprocessing.lowercase,\n",
    "        preprocessing.remove_whitespace,\n",
    "        preprocessing.remove_punctuation,\n",
    "        preprocessing.remove_urls,\n",
    "        preprocessing.remove_brackets,\n",
    "        preprocessing.remove_stopwords,\n",
    "        preprocessing.remove_digits,\n",
    "        preprocessing.remove_angle_brackets,\n",
    "        preprocessing.remove_curly_brackets,\n",
    "        preprocessing.stem\n",
    "        #preprocessing.tokenize\n",
    "    ]\n",
    "\n",
    "    # apply pipeline to text\n",
    "    clean_text = features.pipe(hero.clean, cleaning_pipeline)\n",
    "\n",
    "    return clean_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-27-6466e38200a9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# check processed text\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mclean_text\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpreprocess_text\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mclean_text\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-26-2ad14851f876>\u001B[0m in \u001B[0;36mpreprocess_text\u001B[0;34m()\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0;31m# apply pipeline to text\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m     \u001B[0mclean_text\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfeatures\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhero\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclean\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcleaning_pipeline\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mclean_text\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/ProjectANotebooks/venv/lib/python3.8/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36mpipe\u001B[0;34m(self, func, *args, **kwargs)\u001B[0m\n\u001B[1;32m   5046\u001B[0m         ...  )  # doctest: +SKIP\n\u001B[1;32m   5047\u001B[0m     \"\"\"\n\u001B[0;32m-> 5048\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mcom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   5049\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5050\u001B[0m     _shared_docs[\"aggregate\"] = dedent(\n",
      "\u001B[0;32m~/Desktop/ProjectANotebooks/venv/lib/python3.8/site-packages/pandas/core/common.py\u001B[0m in \u001B[0;36mpipe\u001B[0;34m(obj, func, *args, **kwargs)\u001B[0m\n\u001B[1;32m    469\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    470\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 471\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    472\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/ProjectANotebooks/venv/lib/python3.8/site-packages/texthero/preprocessing.py\u001B[0m in \u001B[0;36mclean\u001B[0;34m(s, pipeline)\u001B[0m\n\u001B[1;32m    386\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    387\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mf\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 388\u001B[0;31m         \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    389\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    390\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/ProjectANotebooks/venv/lib/python3.8/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36mpipe\u001B[0;34m(self, func, *args, **kwargs)\u001B[0m\n\u001B[1;32m   5046\u001B[0m         ...  )  # doctest: +SKIP\n\u001B[1;32m   5047\u001B[0m     \"\"\"\n\u001B[0;32m-> 5048\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mcom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   5049\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5050\u001B[0m     _shared_docs[\"aggregate\"] = dedent(\n",
      "\u001B[0;32m~/Desktop/ProjectANotebooks/venv/lib/python3.8/site-packages/pandas/core/common.py\u001B[0m in \u001B[0;36mpipe\u001B[0;34m(obj, func, *args, **kwargs)\u001B[0m\n\u001B[1;32m    469\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    470\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 471\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    472\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/ProjectANotebooks/venv/lib/python3.8/site-packages/texthero/preprocessing.py\u001B[0m in \u001B[0;36mstem\u001B[0;34m(input, stem, language)\u001B[0m\n\u001B[1;32m    340\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m\" \"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstemmer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    341\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 342\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_stem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    343\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    344\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/ProjectANotebooks/venv/lib/python3.8/site-packages/pandas/core/series.py\u001B[0m in \u001B[0;36mapply\u001B[0;34m(self, func, convert_dtype, args, **kwds)\u001B[0m\n\u001B[1;32m   4210\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4211\u001B[0m                 \u001B[0mvalues\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobject\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4212\u001B[0;31m                 \u001B[0mmapped\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap_infer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconvert\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconvert_dtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4213\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4214\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmapped\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmapped\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mSeries\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/lib.pyx\u001B[0m in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m~/Desktop/ProjectANotebooks/venv/lib/python3.8/site-packages/texthero/preprocessing.py\u001B[0m in \u001B[0;36m_stem\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m    338\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    339\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_stem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 340\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m\" \"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstemmer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    341\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    342\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_stem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/ProjectANotebooks/venv/lib/python3.8/site-packages/texthero/preprocessing.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    338\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    339\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_stem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 340\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m\" \"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstemmer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    341\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    342\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_stem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Desktop/ProjectANotebooks/venv/lib/python3.8/site-packages/nltk/stem/snowball.py\u001B[0m in \u001B[0;36mstem\u001B[0;34m(self, word)\u001B[0m\n\u001B[1;32m   1432\u001B[0m             \u001B[0mword\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Y\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1433\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1434\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1435\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__vowels\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"y\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1436\u001B[0m                 \u001B[0mword\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Y\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# check processed text\n",
    "clean_text = preprocess_text()\n",
    "clean_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(clean_text, target, random_state=0, test_size=0.25, shuffle=True)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# feature extraction methods\n",
    "# tfidf\n",
    "def tfidf():\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', max_features=1000)\n",
    "    vectorizer.fit(clean_text)\n",
    "    with open('TFIDF_1000.pickle', 'wb') as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    train_tfidf = vectorizer.transform(x_train)\n",
    "    test_tfidf = vectorizer.transform(x_test)\n",
    "    return train_tfidf, test_tfidf\n",
    "\n",
    "# bow\n",
    "def bow():\n",
    "    count_vectorizer = CountVectorizer(analyzer='word', max_features=1000)\n",
    "    count_vectorizer.fit(clean_text)\n",
    "    train_bow = count_vectorizer.transform(x_train)\n",
    "    test_bow = count_vectorizer.transform(x_test)\n",
    "    return train_bow, test_bow\n",
    "\n",
    "# bigrams\n",
    "def bigrams():\n",
    "    bigram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2,2), max_features=1000)\n",
    "    bigram_vectorizer.fit(clean_text)\n",
    "    train_bigram = bigram_vectorizer.transform(x_train)\n",
    "    test_bigram = bigram_vectorizer.transform(x_test)\n",
    "    return train_bigram, test_bigram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get features\n",
    "train_tfidf, test_tfidf = tfidf()\n",
    "train_tfidf.shape, test_tfidf.shape\n",
    "\n",
    "train_bigram, test_bigram = bigrams()\n",
    "\n",
    "train_bow, test_bow = bow()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "oversample = SMOTE(random_state=0,n_jobs=-1,k_neighbors=5)\n",
    "train_tfidf, y_train = oversample.fit_resample(train_tfidf, y_train)\n",
    "print(f'Shape: {train_tfidf.shape}')\n",
    "print(y_train.value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# random forest classifier with TFIDF\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# fit\n",
    "rf.fit(train_tfidf, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = rf.predict(test_tfidf)\n",
    "\n",
    "# accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [380, 400, 420, 450, 500],\n",
    "    #'max_depth' : [20,30,35,40,45],\n",
    "    'criterion' :['gini']\n",
    "}\n",
    "CV_rfc = GridSearchCV(estimator=RandomForestClassifier(random_state=0),\n",
    "                      param_grid=param_grid, cv= 5, n_jobs=-1)\n",
    "CV_rfc.fit(train_tfidf, y_train)\n",
    "print(CV_rfc.best_params_)\n",
    "print(CV_rfc.best_score_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8175046554934823\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    AF Other       0.56      0.52      0.54        93\n",
      "      Animal       0.86      0.85      0.86        67\n",
      "   Conductor       0.51      0.68      0.58        34\n",
      "  Connection       0.67      0.82      0.74       210\n",
      "    Crossarm       0.85      0.92      0.88       130\n",
      "      Dug up       0.89      0.92      0.91        92\n",
      "        Fuse       0.86      0.80      0.83       107\n",
      "Installation       0.00      0.00      0.00         8\n",
      "   Lightning       0.97      0.78      0.86        41\n",
      "    OH Cable       0.71      0.65      0.68        60\n",
      "       Other       0.95      0.85      0.90       325\n",
      "        Pole       0.67      0.54      0.60        83\n",
      "       Trees       0.88      0.95      0.91       134\n",
      "    UG Cable       0.00      0.00      0.00         6\n",
      "     Vehicle       0.89      0.94      0.91       221\n",
      "\n",
      "    accuracy                           0.82      1611\n",
      "   macro avg       0.69      0.68      0.68      1611\n",
      "weighted avg       0.82      0.82      0.81      1611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random forest classifier\n",
    "rf = RandomForestClassifier(random_state=0, criterion='gini',  n_estimators=390)\n",
    "\n",
    "# fit\n",
    "rf.fit(train_tfidf, y_train)\n",
    "\n",
    "#with open('random_forest.pickle', 'wb') as f:\n",
    "   # pickle.dump(rf, f)\n",
    "\n",
    "# predict\n",
    "y_pred = rf.predict(test_tfidf)\n",
    "\n",
    "# accuracy\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}