{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix, classification_report\n",
    "\n",
    "# global settings\n",
    "plt.style.use('classic')\n",
    "matplotlib.rcParams['figure.figsize'] = (20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluation metrics for imbalanced data\n",
    "def model_evaluation(model, x_train, y_train, x_test, y_test, model_name, y_pred):\n",
    "    \"\"\":arg\n",
    "    :model, classifier, must have .fit() and .predict()\n",
    "    :x_train, X, features\n",
    "    :y_train, y, target\n",
    "    :x_test, test features\n",
    "    :y_test, test target\n",
    "    :model_name, name of the classifier, string\n",
    "    :y_pred, prediction made by classifier\n",
    "    \"\"\"\n",
    "    # accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f'The accuracy for {model_name}: {acc}')\n",
    "\n",
    "    # F-measure\n",
    "    f1score = f1_score(y_test, y_pred, average='micro')\n",
    "    print(f'F-Measure for {model_name}: {f1score}')\n",
    "\n",
    "    # Recall\n",
    "    recallscore = f1_score(y_test, y_pred, average='micro')\n",
    "    print(f'Recall for {model_name}: {recallscore}')\n",
    "\n",
    "    # classification report\n",
    "    visualizer = ClassificationReport(model, support=True, cmap='spectral')\n",
    "\n",
    "    visualizer.fit(x_train, y_train)        # Fit the visualizer and the model\n",
    "    visualizer.score(x_test, y_test)        # Evaluate the model on the test data\n",
    "    visualizer.show()\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    matrix = plot_confusion_matrix(model, x_test, y_test, cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion matrix for {model_name} classifier')\n",
    "    plt.xticks(rotation=65)\n",
    "    plt.show(matrix)\n",
    "    plt.show()\n",
    "\n",
    "    # precision recall curve\n",
    "    # Create the visualizer, fit, score, and show it\n",
    "    viz = PrecisionRecallCurve(\n",
    "        model,\n",
    "        per_class=True,\n",
    "        classes=np.unique(y_train), # if LabelEncoder used, use encoder.classes_\n",
    "        cmap=\"Set1\",\n",
    "        colors=[\"purple\", \"cyan\", \"slategray\", \"red\", \"grey\", \"yellow\", \"maroon\", \"gold\", \"orange\", \"olive\", \"brown\", \"deeppink\", 'darkkhaki', \"black\", \"navy\"]\n",
    "    )\n",
    "    viz.fit(x_train, y_train)\n",
    "    viz.score(x_test, y_test)\n",
    "    viz.show()\n",
    "\n",
    "    # class prediction error\n",
    "    # Instantiate the classification model and visualizer\n",
    "    visualizer = ClassPredictionError(\n",
    "        model,\n",
    "        colors=[\"purple\", \"cyan\", \"slategray\", \"red\", \"grey\", \"yellow\", \"maroon\", \"gold\", \"orange\", \"olive\", \"brown\", \"deeppink\", 'darkkhaki', \"black\", \"navy\"]\n",
    "    )\n",
    "\n",
    "    # Fit the training data to the visualizer\n",
    "    visualizer.fit(x_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    visualizer.score(x_test, y_test)\n",
    "\n",
    "    # Draw visualization\n",
    "    visualizer.show()\n",
    "\n",
    "    # Create the learning curve visualizer\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "    sizes = np.linspace(0.3, 1.0, 10)\n",
    "\n",
    "    # Instantiate the classification model and visualizer\n",
    "    visualizer = LearningCurve(\n",
    "        model, cv=cv, scoring='f1_micro', train_sizes=sizes, n_jobs=4\n",
    "    )\n",
    "\n",
    "    visualizer.fit(x_train, y_train)        # Fit the data to the visualizer\n",
    "    visualizer.show()\n",
    "\n",
    "    # learning curve with train and test data\n",
    "    plot_learning_curves(x_train, y_train, x_test, y_test, model, train_marker='o', test_marker='^',\n",
    "                     scoring='misclassification error', suppress_plot=False, print_model=True, legend_loc='best')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sample for RF classifier\n",
    "model_evaluation(rf, train_tfidf, y_train, test_tfidf, y_test, 'Random Forest', y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}