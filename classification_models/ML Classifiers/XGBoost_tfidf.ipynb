{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHuv5dqZ-WED",
        "outputId": "d442021c-525d-4a37-d8b6-4d2fee7af2eb"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "\n",
        "import nltk \n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "import pickle\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_obagnI-7_T"
      },
      "source": [
        "# Creating a list for columns to keep\n",
        "cols = ['EventDescription','FailedAssets','IncidentCause','IncidentConsequence','IncidentType','WeatherStation','Status','Category']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kABiHq76AFbE",
        "outputId": "ad2d1b37-5526-4ea5-d370-c6e58676df96"
      },
      "source": [
        "# Importing file\n",
        "df = pd.read_csv('cleaned_incidents1.csv', usecols=cols)\n",
        "print(df.shape)\n",
        "\n",
        "#dropping nulls\n",
        "df = df.dropna()\n",
        "df.isnull().sum()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6504, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EventDescription       0\n",
              "FailedAssets           0\n",
              "IncidentCause          0\n",
              "IncidentConsequence    0\n",
              "IncidentType           0\n",
              "Status                 0\n",
              "WeatherStation         0\n",
              "Category               0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBkL3cC1AP7Z"
      },
      "source": [
        "df['Description'] = df['WeatherStation'] + ' ' + df['IncidentType'] + ' ' + df['Status'] + ' ' + df['EventDescription'] + ' ' + df['FailedAssets'] + ' ' + df['IncidentCause']+ ' ' + df['IncidentConsequence']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roQKB5o4AetT",
        "outputId": "da76e011-b33a-4f6a-9c1a-a26ea2405ba6"
      },
      "source": [
        "print(df['Description'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       Avalon Airport Infrastructure (network-based) ...\n",
            "1       Laverton Raaf Infrastructure (network-based) R...\n",
            "2       Warrnambool Airport Ndb Infrastructure (networ...\n",
            "3       Essendon Airport Infrastructure (network-based...\n",
            "4       Avalon Airport Infrastructure (network-based) ...\n",
            "                              ...                        \n",
            "6499    Moorabbin Airport Infrastructure (network-base...\n",
            "6500    Avalon Airport Infrastructure (network-based) ...\n",
            "6501    Mildura Airport Infrastructure (network-based)...\n",
            "6502    Swan Hill Aerodrome Infrastructure (network-ba...\n",
            "6503    Moorabbin Airport Infrastructure (network-base...\n",
            "Name: Description, Length: 6488, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzhaWokIA0ki"
      },
      "source": [
        "Stopwords, Splitting, Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "5vKtiUShA1ln",
        "outputId": "2bace0b4-eb69-4ff1-97a5-ecb577bec76d"
      },
      "source": [
        "# Creating stopwords list\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "numerical_symbols = re.compile('0-90-9a-z')\n",
        " \n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        " \n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = numerical_symbols.sub('', text)\n",
        "    text = text.replace('x', '')\n",
        "    #text = re.sub(r'\\W+', '', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text\n",
        "\n",
        "df['Description'] = df['Description'].apply(clean_text)\n",
        "\n",
        "\n",
        "'''\n",
        "def getLemmText(text):\n",
        "    tokens=word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
        "    \n",
        "    #ps = PorterStemmer()\n",
        "    #tokens=[ps.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "df['Description'] = list(map(getLemmText,df['Description']))\n",
        "\n",
        "\n",
        "def getStemmText(text):\n",
        "    tokens=word_tokenize(text)\n",
        "    ps = PorterStemmer()\n",
        "    tokens=[ps.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "df['Description'] = list(map(getStemmText,df['Description']))\n",
        "\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef getLemmText(text):\\n    tokens=word_tokenize(text)\\n    lemmatizer = WordNetLemmatizer()\\n    tokens=[lemmatizer.lemmatize(word) for word in tokens]\\n    \\n    #ps = PorterStemmer()\\n    #tokens=[ps.stem(word) for word in tokens]\\n    return ' '.join(tokens)\\ndf['Description'] = list(map(getLemmText,df['Description']))\\n\\n\\ndef getStemmText(text):\\n    tokens=word_tokenize(text)\\n    ps = PorterStemmer()\\n    tokens=[ps.stem(word) for word in tokens]\\n    return ' '.join(tokens)\\ndf['Description'] = list(map(getStemmText,df['Description']))\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKsbSrkxBIxQ",
        "outputId": "5aa01a19-7ea0-4968-e118-945d70f6d465"
      },
      "source": [
        "print(df['Description'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       avalon airport infrastructure networkbased rep...\n",
            "1       laverton raaf infrastructure networkbased repo...\n",
            "2       warrnambool airport ndb infrastructure network...\n",
            "3       essendon airport infrastructure networkbased r...\n",
            "4       avalon airport infrastructure networkbased rep...\n",
            "                              ...                        \n",
            "6499    moorabbin airport infrastructure networkbased ...\n",
            "6500    avalon airport infrastructure networkbased rep...\n",
            "6501    mildura airport infrastructure networkbased re...\n",
            "6502    swan hill aerodrome infrastructure networkbase...\n",
            "6503    moorabbin airport infrastructure networkbased ...\n",
            "Name: Description, Length: 6488, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKnwWr7oBWZI"
      },
      "source": [
        "# Label encoding for Category\n",
        "le = LabelEncoder()\n",
        "df['Category'] = le.fit_transform(df['Category'].astype(str))\n",
        "#store the 'Category' variable in Y\n",
        "X = df[['Description']]\n",
        "Y = df[['Category']]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMx6FiYDBaYM"
      },
      "source": [
        "# Splitting of data in test and train\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['Description'],Y, \n",
        "                                                    test_size=0.25, random_state=4)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9RTvGhfCtYp"
      },
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv2PN9wSLcbb"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rxb30QYLa4m"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=stopwords, analyzer='word', max_features=1000)\n",
        "tfidf = vectorizer.fit(df['Description'])\n",
        "# tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Buwz5BCjMEAR"
      },
      "source": [
        "x_train_tfidf =  tfidf.transform(x_train)\n",
        "x_test_tfidf =  tfidf.transform(x_test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAFQg8XUd9Jq",
        "outputId": "eb4eb902-d2e4-4f06-f0d4-f87604400790"
      },
      "source": [
        "!pip install numpy Cython\n",
        "\n",
        "!pip install pymrmr"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.21)\n",
            "Collecting pymrmr\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/ab/903712947a2f5cd1af249132885dbd81ae8bf8cfd30fb3b3f2beddab23e8/pymrmr-0.1.8.tar.gz (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pymrmr) (1.18.5)\n",
            "Building wheels for collected packages: pymrmr\n",
            "  Building wheel for pymrmr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymrmr: filename=pymrmr-0.1.8-cp36-cp36m-linux_x86_64.whl size=256734 sha256=ac5d7007350717be229cbeae9582daf19e10a7a8434e15f3a597e9901d1ea350\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ce/3a/bc9b80047f68973d909a35bb8e3062b7c7377510607ec35998\n",
            "Successfully built pymrmr\n",
            "Installing collected packages: pymrmr\n",
            "Successfully installed pymrmr-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3EKEAqqgZes"
      },
      "source": [
        "# df_mrmr = pd.DataFrame(x_train_tfidf)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_trIuVjdbwl"
      },
      "source": [
        "# import pymrmr\n",
        "\n",
        "# pymrmr.mRMR(df_mrmr, 'MID', 7)\n",
        "# # print(df_mrmr)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c586ReNpVoN"
      },
      "source": [
        "# print(x_train_tfidf)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azZAGayVCvNv"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "clf = XGBClassifier(learning_rate=0.1, verbosity=2)\n",
        "clf.fit(x_train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMUjJb4yUIy3",
        "outputId": "8ff0a917-c04a-4e20-f0d4-6e1a681a3c13"
      },
      "source": [
        "# Model evaluation\n",
        "prediction = clf.predict(x_test_tfidf)\n",
        "\n",
        "acc = accuracy_score(y_test, prediction).round(4)\n",
        "print(\"Accuracy using TF-IDF is: {}%\".format(acc * 100.0))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy using TF-IDF is: 81.81%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-avg4UQBf3C"
      },
      "source": [
        "Hyper-parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgcmOYe4BiEE",
        "outputId": "86a87dac-f569-49b5-822b-17d436700781"
      },
      "source": [
        "# grid search\n",
        "param_grid = {\n",
        "    'learning_rate': [0.10, 0.20, 0.50],\n",
        "    'max_depth' : [3, 4],\n",
        "    'verbosity' : [1, 2],\n",
        "    'min_child_weight' :[1, 2],\n",
        "    # 'gamma' : [0.1, 0.2],\n",
        "    # 'colsample_bytree' : [0.3, 0.4, 0.5, 0.7]\n",
        "}\n",
        "cv_xgb = GridSearchCV(estimator=XGBClassifier(random_state=4),\n",
        "                      param_grid=param_grid, cv= 5, n_jobs=-1)\n",
        "cv_xgb.fit(x_train_tfidf, y_train)\n",
        "cv_xgb.best_params_\n",
        "print(cv_xgb.best_score_)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[06:25:20] ======== Monitor: Learner ========\n",
            "[06:25:20] ======== Monitor: GBTree ========\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.836826766219761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV6TVfM6um4I",
        "outputId": "6ee8c6d2-1244-455b-da7c-425665b208d9"
      },
      "source": [
        "print(cv_xgb.best_params_)\n",
        "#Best Parameters : {learning_rate : 0.2, max_depth: 4, min_child_weight: 1, verbosity: 1}"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'learning_rate': 0.2, 'max_depth': 4, 'min_child_weight': 1, 'verbosity': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb1iVWthwor1"
      },
      "source": [
        "Optimized Model with Best Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVmqPvGeU8q1",
        "outputId": "e673f2ea-f4d6-4f27-843b-0fc6f844733f"
      },
      "source": [
        "# clf = XGBClassifier(learning_rate=0.5, max_depth=3, gamma= 0.1, min_child_weight=5, verbosity=3)\n",
        "clf = XGBClassifier(learning_rate=0.2, max_depth=4, verbosity=1, min_child_weight=1)\n",
        "clf.fit(x_train_tfidf, y_train)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.2, max_delta_step=0, max_depth=4,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='multi:softprob', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsLvf-PAU_la",
        "outputId": "7c545b06-c464-42ad-c956-fd48191e2b51"
      },
      "source": [
        "prediction = clf.predict(x_test_tfidf)\n",
        "\n",
        "acc = accuracy_score(y_test, prediction).round(4)\n",
        "print(\"Accuracy using TF-IDF is: {}%\".format(acc * 100.0))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy using TF-IDF is: 82.31%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}