{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayes_tfidf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toE5M14ozhg9",
        "outputId": "7f8245b5-0d50-4ddd-f3f8-f1af0c988295"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "\n",
        "import nltk \n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "import pickle\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJqSBdYh0DlD"
      },
      "source": [
        "# Creating a list for columns to keep\n",
        "cols = ['EventDescription','FailedAssets','IncidentCause','IncidentConsequence','IncidentType','WeatherStation','Status','Category']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCjoH5AU0EbB",
        "outputId": "207f4db5-aa54-4ce8-b07a-57720e5ec735"
      },
      "source": [
        "# Importing file\n",
        "df = pd.read_csv('cleaned_incidents1.csv', usecols=cols)\n",
        "print(df.shape)\n",
        "\n",
        "#dropping nulls\n",
        "df = df.dropna()\n",
        "df.isnull().sum()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6504, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EventDescription       0\n",
              "FailedAssets           0\n",
              "IncidentCause          0\n",
              "IncidentConsequence    0\n",
              "IncidentType           0\n",
              "Status                 0\n",
              "WeatherStation         0\n",
              "Category               0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL-C_Djm0NN6"
      },
      "source": [
        "df['Description'] = df['WeatherStation'] + ' ' + df['IncidentType'] + ' ' + df['Status'] + ' ' + df['EventDescription'] + ' ' + df['FailedAssets'] + ' ' + df['IncidentCause']+ ' ' + df['IncidentConsequence']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lccH7c0-0N22",
        "outputId": "c57a0dce-9eea-4d17-aab5-756bf413c02c"
      },
      "source": [
        "print(df['Description'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       Avalon Airport Infrastructure (network-based) ...\n",
            "1       Laverton Raaf Infrastructure (network-based) R...\n",
            "2       Warrnambool Airport Ndb Infrastructure (networ...\n",
            "3       Essendon Airport Infrastructure (network-based...\n",
            "4       Avalon Airport Infrastructure (network-based) ...\n",
            "                              ...                        \n",
            "6499    Moorabbin Airport Infrastructure (network-base...\n",
            "6500    Avalon Airport Infrastructure (network-based) ...\n",
            "6501    Mildura Airport Infrastructure (network-based)...\n",
            "6502    Swan Hill Aerodrome Infrastructure (network-ba...\n",
            "6503    Moorabbin Airport Infrastructure (network-base...\n",
            "Name: Description, Length: 6488, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MOK4OZ30UAd"
      },
      "source": [
        "\n",
        "Stopwords, Splitting, Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "1x0qt3p80QpP",
        "outputId": "fbd04c1b-2e9b-4dd9-a876-ab104c3d8d4d"
      },
      "source": [
        "# Creating stopwords list\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "numerical_symbols = re.compile('0-90-9a-z')\n",
        " \n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        " \n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
        "    text = numerical_symbols.sub('', text)\n",
        "    text = text.replace('x', '')\n",
        "    #text = re.sub(r'\\W+', '', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
        "    return text\n",
        "\n",
        "df['Description'] = df['Description'].apply(clean_text)\n",
        "\n",
        "\n",
        "'''\n",
        "def getLemmText(text):\n",
        "    tokens=word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
        "    \n",
        "    #ps = PorterStemmer()\n",
        "    #tokens=[ps.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "df['Description'] = list(map(getLemmText,df['Description']))\n",
        "\n",
        "\n",
        "def getStemmText(text):\n",
        "    tokens=word_tokenize(text)\n",
        "    ps = PorterStemmer()\n",
        "    tokens=[ps.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "df['Description'] = list(map(getStemmText,df['Description']))\n",
        "\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef getLemmText(text):\\n    tokens=word_tokenize(text)\\n    lemmatizer = WordNetLemmatizer()\\n    tokens=[lemmatizer.lemmatize(word) for word in tokens]\\n    \\n    #ps = PorterStemmer()\\n    #tokens=[ps.stem(word) for word in tokens]\\n    return ' '.join(tokens)\\ndf['Description'] = list(map(getLemmText,df['Description']))\\n\\n\\ndef getStemmText(text):\\n    tokens=word_tokenize(text)\\n    ps = PorterStemmer()\\n    tokens=[ps.stem(word) for word in tokens]\\n    return ' '.join(tokens)\\ndf['Description'] = list(map(getStemmText,df['Description']))\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFGpGET_0dfO",
        "outputId": "b83961b9-dc3f-45c2-eb5f-07cf7742de40"
      },
      "source": [
        "print(df['Description'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       avalon airport infrastructure networkbased rep...\n",
            "1       laverton raaf infrastructure networkbased repo...\n",
            "2       warrnambool airport ndb infrastructure network...\n",
            "3       essendon airport infrastructure networkbased r...\n",
            "4       avalon airport infrastructure networkbased rep...\n",
            "                              ...                        \n",
            "6499    moorabbin airport infrastructure networkbased ...\n",
            "6500    avalon airport infrastructure networkbased rep...\n",
            "6501    mildura airport infrastructure networkbased re...\n",
            "6502    swan hill aerodrome infrastructure networkbase...\n",
            "6503    moorabbin airport infrastructure networkbased ...\n",
            "Name: Description, Length: 6488, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6In4QSd0gdq"
      },
      "source": [
        "# Label encoding for Category\n",
        "le = LabelEncoder()\n",
        "df['Category'] = le.fit_transform(df['Category'].astype(str))\n",
        "#store the 'Category' variable in Y\n",
        "X = df[['Description']]\n",
        "Y = df[['Category']]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZD4ThhZ0lVw"
      },
      "source": [
        "# Splitting of data in test and train\n",
        "x_train, x_test, y_train, y_test = train_test_split(df['Description'],Y, \n",
        "                                                    test_size=0.2, random_state=4)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffBlMCCU0nZX"
      },
      "source": [
        "Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AACVoKxE0oz4"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfhANP7m0udn"
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=stopwords, analyzer='word')\n",
        "tfidf = vectorizer.fit(df['Description'])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCFEFRet0wTI"
      },
      "source": [
        "x_train_tfidf =  tfidf.transform(x_train)\n",
        "x_test_tfidf =  tfidf.transform(x_test)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51yRkDF90zQR",
        "outputId": "1accdc8d-c2bb-4524-9516-6f0a9f7edfb4"
      },
      "source": [
        "from sklearn.naive_bayes import ComplementNB\n",
        "\n",
        "clf = ComplementNB()\n",
        "# xtrain = x_train_tfidf.todense()\n",
        "\n",
        "clf.fit(x_train_tfidf, y_train)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIgwMaBE3VJD",
        "outputId": "7ff930d6-12eb-40f1-ac4a-8fa190c7781c"
      },
      "source": [
        "# Model evaluation\n",
        "# xtest = x_test_tfidf.todense()\n",
        "prediction = clf.predict(x_test_tfidf)\n",
        "acc = accuracy_score(y_test, prediction).round(4)\n",
        "print(\"Accuracy using TF-IDF is: {}%\".format(acc * 100.0))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy using TF-IDF is: 73.42%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}