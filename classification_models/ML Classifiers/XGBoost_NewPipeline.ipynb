{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost_NewPipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA8LfpobOGNG"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf5tl89iwZiX",
        "outputId": "029320fa-873f-4f45-b41f-9c176db66cf3"
      },
      "source": [
        "# import libraries\n",
        "!pip install texthero\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import string\n",
        "    import re\n",
        "    from time import time\n",
        "    import texthero as hero\n",
        "    from texthero import preprocessing\n",
        "    from gensim.models import Word2Vec\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.ensemble import AdaBoostClassifier\n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.ensemble import StackingClassifier\n",
        "    from sklearn.svm import SVC\n",
        "    import warnings\n",
        "except(ImportError):\n",
        "    print(f'Import Error: {ImportError}')\n",
        "\n",
        "# ignore warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# set seeds for reproducability\n",
        "from numpy.random import seed\n",
        "seed(500)\n",
        "\n",
        "# global configurations\n",
        "pd.set_option(\"display.max_colwidth\", -1) # show larger text in pandas dataframe"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting texthero\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/5a/a9d33b799fe53011de79d140ad6d86c440a2da1ae8a7b24e851ee2f8bde8/texthero-1.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from texthero) (3.2.2)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from texthero) (2.2.4)\n",
            "Requirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from texthero) (1.5.0)\n",
            "Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from texthero) (4.4.1)\n",
            "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.6/dist-packages (from texthero) (4.41.1)\n",
            "Collecting unidecode>=1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.6/dist-packages (from texthero) (0.22.2.post1)\n",
            "Collecting nltk>=3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 31.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from texthero) (1.1.4)\n",
            "Requirement already satisfied: gensim>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from texthero) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from texthero) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->texthero) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->texthero) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->texthero) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (1.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (0.8.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (1.0.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (50.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (2.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (3.0.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud>=1.5.0->texthero) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly>=4.2.0->texthero) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=4.2.0->texthero) (1.3.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.22->texthero) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.22->texthero) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk>=3.3->texthero) (2019.12.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.6.0->texthero) (3.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (3.4.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434676 sha256=78a94ac7c3682794c5eb203bb8ef576183f67f0725b409e38dffe55f42eb92e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: unidecode, nltk, texthero\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5 texthero-1.0.9 unidecode-1.1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SlJl3yMwyQh"
      },
      "source": [
        "'''\n",
        "- select features\n",
        "- drop missing values\n",
        "- combine selected features into one\n",
        "'''\n",
        "def data(df):\n",
        "    new_df = df[['EventDescription', 'FailedAssets', 'IncidentCause', 'IncidentConsequence', 'IncidentType', 'Status', 'WeatherStation', 'Category']]\n",
        "    new_df.dropna(axis=0, inplace=True)\n",
        "    features = new_df['IncidentType'] + ' ' + new_df['Status'] + ' ' + new_df['WeatherStation'] + new_df['EventDescription'] + new_df['FailedAssets'] + new_df['IncidentCause'] + new_df['IncidentConsequence']\n",
        "    target = new_df['Category']\n",
        "    return features, target"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjmBk2z8w49C"
      },
      "source": [
        "# read csv\n",
        "dataset = pd.read_csv(\"cleaned_incidents1.csv\")\n",
        "\n",
        "# get features and target\n",
        "features, target = data(dataset)\n",
        "\n",
        "# Text Cleaning and Pre-processing\n",
        "def preprocess_text():\n",
        "    # cleaning steps\n",
        "    cleaning_pipeline = [\n",
        "        preprocessing.fillna,\n",
        "        preprocessing.lowercase,\n",
        "        preprocessing.remove_whitespace,\n",
        "        preprocessing.remove_punctuation,\n",
        "        preprocessing.remove_urls,\n",
        "        preprocessing.remove_brackets,\n",
        "        preprocessing.remove_stopwords,\n",
        "        preprocessing.remove_digits,\n",
        "        preprocessing.remove_angle_brackets,\n",
        "        preprocessing.remove_curly_brackets,\n",
        "        preprocessing.stem\n",
        "        #preprocessing.tokenize\n",
        "    ]\n",
        "\n",
        "    # apply pipeline to text\n",
        "    clean_text = features.pipe(hero.clean, cleaning_pipeline)\n",
        "\n",
        "    return clean_text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzv1rOd0xcR3",
        "outputId": "b6d65a35-2f3e-46d3-c786-bb4e4f71d9bc"
      },
      "source": [
        "# check processed text\n",
        "clean_text = preprocess_text()\n",
        "clean_text"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       infrastructur network base report avalon airporta nearbi custom report spark electr line locat attend crew found high voltag abc conductor fault midspan result ground fire approx sqm report injuri conductor abc hvabc cabl fault midspangrassfir                                                                                                                                                                                                                               \n",
              "1       infrastructur network base report laverton raafa contractor report contact earth cabl excav trench locat arriv crew found contractor denni jame ph dig trench contact earth cabl caus damag cabl report injuri third parti properti damag fail assetcontractor contact earth conductorno go zone contact                                                                                                                                                                          \n",
              "2       infrastructur network base report warrnambool airport ndba field crew attend outag found 22kv conductor broken due rust connect sleev fallen ground one end remain aliv due high imped backfe downlin transform wind protect oper report injuri third parti properti damag conductor bare conductor broke due rustseri risk public safeti e g live conductor ground live asset access unauthoris person                                                                           \n",
              "3       infrastructur network base report essendon airportinterfer vandal substat unknown third parti use unknown object interfer two hv fuse blew indoor substat littl bank kingsway indoor substat affect also secur park rear bank street theft vandal ticket machin conjunct substat interfer vandal indoorvandalismloss suppli damag network asset                                                                                                                                   \n",
              "4       infrastructur network base report avalon airporta nearbi custom report high load pull wire locat arriv crew found lv servic cabl contact unknown vehicl break cabl fell ground report injuri third parti properti damag follow repair cabl measur 3m kerb servic conductorunknown high load contact lv servic cableno go zone contact damag network asset                                                                                                                         \n",
              "                                                                                                                                                                                  ...                                                                                                                                                                                                                                                                                                     \n",
              "6499    infrastructur network base report moorabbin airporta report came ue fmb spark st kilda st brighton arriv crew identifi fire damag folcb pole lis eio call assess caus connect boxth like caus determin eio earth defect within custom instal eio issu defect st kilda stplant equip                                                                                                                                                                                               \n",
              "6500    infrastructur network base report avalon airportreport receiv resid advis crane made contact powerlin power wire appear fine crane longer contact upon arriv fault crew found crane driver metcalf crane servic made contact 12st swer overhead conductor lis lis move gantri equip associ moorabool windfarm project crane taken away site test fire report injuri conductor bare crane struck lineno go zone contact damag network asset                                        \n",
              "6501    infrastructur network base report mildura airportreport receiv pole fire incid locat arriv crew found top burnt swer pole fire extinguish ground fire report injuri third parti properti damag pole wood excess leakag currentpol top fire                                                                                                                                                                                                                                        \n",
              "6502    infrastructur network base report swan hill aerodromea custom call report tractor hit wire brought ground arriv incid locat crew found tractor rego xv 99ce tow larg roller around paddock turn sharpli contact pole roller caus break fall 22kv hv singl phase conductor fell onto tractor caus protect oper isol suppli tractor driver exit vehicl crew arriv report receiv shock injur report damag fire injuri conductor bare pole wood farm equip hit poledamag network asset\n",
              "6503    infrastructur network base notif moorabbin airportconcret electr cover outsid front hous gave client dog electr shock rainservic pit network investigationshock livestock pet companion anim                                                                                                                                                                                                                                                                                      \n",
              "Length: 6488, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZCk1zQMxhAA",
        "outputId": "2f2ebef8-00e9-495f-a45d-78051a0b0fe9"
      },
      "source": [
        "# train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(clean_text, target, random_state=0, test_size=0.25, shuffle=True)\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4866,), (1622,), (4866,), (1622,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td_Z_qG0xl7b"
      },
      "source": [
        "# feature extraction methods\n",
        "# tfidf\n",
        "def tfidf():\n",
        "    vectorizer = TfidfVectorizer(analyzer='word', max_features=1000)\n",
        "    vectorizer.fit(clean_text)\n",
        "    train_tfidf = vectorizer.transform(x_train)\n",
        "    test_tfidf = vectorizer.transform(x_test)\n",
        "    return train_tfidf, test_tfidf\n",
        "\n",
        "# bow\n",
        "def bow():\n",
        "    count_vectorizer = CountVectorizer(analyzer='word', max_features=1000)\n",
        "    count_vectorizer.fit(clean_text)\n",
        "    train_bow = count_vectorizer.transform(x_train)\n",
        "    test_bow = count_vectorizer.transform(x_test)\n",
        "    return train_bow, test_bow\n",
        "\n",
        "# bigrams\n",
        "def bigrams():\n",
        "    bigram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(2,2), max_features=1000)\n",
        "    bigram_vectorizer.fit(clean_text)\n",
        "    train_bigram = bigram_vectorizer.transform(x_train)\n",
        "    test_bigram = bigram_vectorizer.transform(x_test)\n",
        "    return train_bigram, test_bigram"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bquWGx1Cxo7Z"
      },
      "source": [
        "# get features\n",
        "train_tfidf, test_tfidf = tfidf()\n",
        "train_tfidf.shape, test_tfidf.shape\n",
        "\n",
        "train_bigram, test_bigram = bigrams()\n",
        "\n",
        "train_bow, test_bow = bow()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSVWbZzDxwHJ"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "clf = XGBClassifier(learning_rate=0.5, verbosity=2, random_state=4)\n",
        "clf.fit(train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGGFuUChy2bc",
        "outputId": "f8b4681a-3da5-4ce1-93d1-686d0618db5d"
      },
      "source": [
        "# Model evaluation\n",
        "prediction = clf.predict(test_tfidf)\n",
        "\n",
        "acc = accuracy_score(y_test, prediction).round(4)\n",
        "print(\"Accuracy using TF-IDF is: {}%\".format(acc * 100.0))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy using TF-IDF is: 80.89%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eslkkWFp2nmF",
        "outputId": "3e7cbd9a-d1a0-4145-ad1e-1449c5e2e750"
      },
      "source": [
        "# grid search\n",
        "param_grid = {\n",
        "    'learning_rate': [0.10, 0.30, 0.50],\n",
        "    'max_depth' : [10, 20, 30],\n",
        "    'min_child_weight' :[3, 5],\n",
        "    'gamma' : [0.1, 0.2],\n",
        "    # 'colsample_bytree' : [0.3, 0.4, 0.5, 0.7]\n",
        "}\n",
        "cv_xgb = GridSearchCV(estimator=XGBClassifier(random_state=4),\n",
        "                      param_grid=param_grid, cv= 5, n_jobs=-1)\n",
        "cv_xgb.fit(train_tfidf, y_train)\n",
        "cv_xgb.best_params_"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 30, 'min_child_weight': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpaYipLU9wH1"
      },
      "source": [
        "#Best Parameters = gamma:0.1, learning_rate_0.1, max_depth: 30, min_child_weight: 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koHSuLwQ4dIc"
      },
      "source": [
        "# clf1 = XGBClassifier(gamma=0.1, learning_rate=0.1, max_depth=30, min_child_weight=3, random_state=4)\n",
        "clf = XGBClassifier(gamma=0.4, learning_rate=0.05, max_depth=30, verbosity=2, random_state=4)\n",
        "clf.fit(train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6HgVmA05aVb",
        "outputId": "e6803182-f30b-417f-ff04-836dc2410ae0"
      },
      "source": [
        "# Model evaluation\n",
        "prediction = clf.predict(test_tfidf)\n",
        "\n",
        "acc = accuracy_score(y_test, prediction).round(4)\n",
        "print(\"Accuracy using TF-IDF is: {}%\".format(acc * 100.0))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy using TF-IDF is: 81.44%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}